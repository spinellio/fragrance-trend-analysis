# ======================================================================
# Fragrance Case Study â€” End-to-End R Pipeline
# Author: Olivia Spinelli
# ======================================================================
# What this script does:
# 1) Load & tidy Fragrantica data -> note & category supply (counts).
# 2) Load & process Google Trends (notes + categories) monthly series:
#    - % growth (US, UK), slope, normalized slope, baseline interest
# 3) Compute TMS (US, UK, Both) using baseline-adjusted method:
#       TMS = w_g * norm_growth + w_b * norm_baseline (+ w_s * norm_buzz if present)
# 4) Compute Opportunity Scores (US, UK, Both):
#       Opp = TMS * (1 - supply_norm)
# 5) Save tidy outputs and print Top 10 opportunities for deck
# ======================================================================

suppressPackageStartupMessages({
  library(tidyverse)
  library(readr)
  library(janitor)
  library(lubridate)
  library(scales)
  library(stringr)
  library(stringi)
  library(broom)      # for lm() tidy slope
})

# ----------------------------- Paths ----------------------------------
ROOT <- getwd()
DATA <- file.path(ROOT, "data")
OUT  <- file.path(ROOT, "outputs")
dir.create(OUT, showWarnings = FALSE, recursive = TRUE)

# Input files (place them under ./data)
FRAG_CSV          <- file.path(DATA, "Fragrantica_Cleaned.csv")
NOTES_MASTER_CSV  <- file.path(DATA, "Fragrance_Notes_Master.csv")
CATS_MASTER_CSV   <- file.path(DATA, "Fragrance_Categories_Master .csv")  # note: space before .csv
GT_NOTES_CSV      <- file.path(DATA, "GT_Notes_Merged_Monthly.csv")
GT_CATS_CSV       <- file.path(DATA, "GT_Categories_Merged_Monthly.csv")

# ------------------------ Config / Parameters --------------------------
recent_cutoff_year <- 2023     # supply counts from recent launches (adjust if needed)
trend_window_months <- 12      # compute growth/slope over last N months
weights <- list(
  # Option 2 (baseline-adjusted). If social buzz exists, it will be included.
  w_growth = 0.55,
  w_baseline = 0.35,
  w_buzz = 0.10
)

# --------------------------- Utilities ---------------------------------
safe_rescale <- function(x) {
  if (all(is.na(x))) return(rep(NA_real_, length(x)))
  rng <- range(x, na.rm = TRUE)
  if (diff(rng) == 0) return(rep(0.5, length(x)))  # constant -> neutral mid
  rescale(x, to = c(0, 1), from = rng)
}

normalize_text <- function(x) {
  x %>%
    str_replace_all("\\[|\\]|\\(|\\)|'|\"", "") %>%
    str_to_lower() %>%
    str_squish() %>%
    str_remove("^and\\s+") %>%
    str_remove("\\sand$") %>%
    stringi::stri_trans_general("Latin-ASCII")
}

# Canonical recodes (merge variants)
canon_map <- c(
  "mandarin orange" = "mandarin",
  "jasmine sambac"  = "jasmine",
  "damask rose"     = "rose",
  "rose de mai"     = "rose",
  "agarwood"        = "oud",
  "oud wood"        = "oud",
  "labdanum"        = "amber",
  "tonka beans"     = "tonka bean",
  "white musks"     = "white musk",
  "cashmere wood"   = "cashmeran",
  "pink peppers"    = "pink pepper",
  "black peppers"   = "black pepper",
  "green apples"    = "green apple",
  "apples"          = "apple",
  "pear accord"     = "pear",
  "vanilla absolute"= "vanilla",
  "madagascar vanilla" = "vanilla",
  "haitian vetiver" = "vetiver",
  "java vetiver"    = "vetiver",
  "bergamote"       = "bergamot",
  "lily of the valley" = "lily-of-the-valley"
)

# Note -> Category mapping (taxonomy used throughout)
note_categories <- tribble(
  ~note, ~category,
  # citrus
  "bergamot","citrus","lemon","citrus","grapefruit","citrus","mandarin","citrus","orange","citrus",
  # floral
  "jasmine","floral","rose","floral","peony","floral","tuberose","floral","lily-of-the-valley","floral",
  "iris","floral","neroli","floral","freesia","floral","gardenia","floral","violet","floral",
  # fruity
  "apple","fruity","pear","fruity","peach","fruity","raspberry","fruity","black currant","fruity",
  # woody
  "cedar","woody","sandalwood","woody","vetiver","woody","oakmoss","woody","moss","woody","cashmeran","woody","oud","woody",
  # musky
  "musk","musky","white musk","musky",
  # amber
  "amber","amber","ambergris","amber","labdanum","amber",
  # spicy
  "cardamom","spicy","ginger","spicy","saffron","spicy","cinnamon","spicy","nutmeg","spicy","pink pepper","spicy","black pepper","spicy",
  # gourmand
  "vanilla","gourmand","tonka bean","gourmand","chocolate","gourmand","caramel","gourmand","honey","gourmand",
  # green
  "green notes","green","galbanum","green","fig leaf","green","violet leaf","green",
  # leather
  "leather","leather"
)

clean_entity <- function(x) {
  x %>% normalize_text() %>% str_replace("\\s*cat$", "") %>% str_squish()
}

# Percent change on a time series (handle 0 start by taking earliest non-zero)
pct_change_safely <- function(v) {
  v <- as.numeric(v)
  if (all(is.na(v))) return(NA_real_)
  idx <- which(!is.na(v) & v != 0)
  if (length(idx) == 0) return(NA_real_)
  start <- v[min(idx)]
  end   <- v[max(which(!is.na(v)))]
  if (is.na(start) || is.na(end) || start == 0) return(NA_real_)
  (end - start) / start * 100
}

# Linear slope (per month) of raw values
slope_raw <- function(dates, values) {
  ok <- !is.na(dates) & !is.na(values)
  if (sum(ok) < 2) return(NA_real_)
  x <- as.numeric(as.Date(dates[ok]))
  y <- as.numeric(values[ok])
  coef(lm(y ~ x))[2]
}

# Slope on normalized-to-zero (first month aligned to 0)
slope_nz <- function(dates, values) {
  ok <- !is.na(dates) & !is.na(values)
  if (sum(ok) < 2) return(NA_real_)
  y <- as.numeric(values[ok])
  y0 <- y - y[1]
  x <- as.numeric(as.Date(dates[ok]))
  coef(lm(y0 ~ x))[2]
}

# ------------------------ 1) Supply from Fragrantica -------------------
message("Loading Fragrantica...")
fra <- readr::read_csv(FRAG_CSV, locale = locale(encoding = "LATIN1"), show_col_types = FALSE) %>%
  clean_names()

# Expect columns: url, perfume, brand, year, top, middle, base, ...
stopifnot(all(c("url","top","middle","base") %in% names(fra)))

# Ensure year numeric
if ("year" %in% names(fra)) {
  fra <- fra %>%
    mutate(year = suppressWarnings(as.integer(str_extract(as.character(year), "\\d{4}"))))
}

# Filter to recent launches for supply signal (adjustable)
fra_recent <- if ("year" %in% names(fra)) {
  fra %>% filter(!is.na(year), year >= recent_cutoff_year)
} else fra

# Tidy notes long
notes_long <- fra_recent %>%
  select(url, top, middle, base) %>%
  pivot_longer(c(top, middle, base), names_to = "note_type", values_to = "notes_raw") %>%
  separate_rows(notes_raw, sep = "\\s*[,;]\\s*") %>%
  mutate(note = normalize_text(notes_raw)) %>%
  filter(!is.na(note), note != "", note != "none") %>%
  mutate(note = dplyr::recode(note, !!!canon_map)) %>%
  left_join(note_categories, by = "note")

# Supply counts
note_supply <- notes_long %>%
  distinct(url, note, note_type) %>%
  count(note, name = "supply_count_note") %>%
  arrange(desc(supply_count_note))

cat_supply <- notes_long %>%
  distinct(url, category) %>%
  filter(!is.na(category)) %>%
  count(category, name = "supply_count_category") %>%
  arrange(desc(supply_count_category))

# Save supply
write_csv(note_supply, file.path(OUT, "supply_notes_recent.csv"))
write_csv(cat_supply,  file.path(OUT, "supply_categories_recent.csv"))

# ------------------ 2) Google Trends: Notes & Categories ---------------
# Assumed schema (robust rename):
# columns: entity/note/category, market (US/UK), date (YYYY-MM-DD), interest (0-100)

standardize_gt <- function(path, entity_label = c("note","category")) {
  entity_label <- match.arg(entity_label)
  df <- read_csv(path, show_col_types = FALSE) %>% clean_names()
  # Try to find columns
  nm <- names(df)
  col_entity <- nm[str_detect(nm, "entity|note|category|key")]
  col_market <- nm[str_detect(nm, "market|region|country")]
  col_date   <- nm[str_detect(nm, "date")]
  col_val    <- nm[str_detect(nm, "interest|value|score")]

  stopifnot(length(col_entity) >= 1, length(col_market) >= 1, length(col_date) >= 1, length(col_val) >= 1)

  df %>%
    transmute(
      entity = clean_entity(!!sym(col_entity[1])),
      entity_type = entity_label,
      market = str_to_upper(!!sym(col_market[1])),
      date = as.Date(!!sym(col_date[1])),
      interest = as.numeric(!!sym(col_val[1]))
    ) %>%
    filter(market %in% c("US","UK")) %>%
    arrange(entity, market, date)
}

message("Loading GT notes & categories...")
gt_notes <- standardize_gt(GT_NOTES_CSV, "note")
gt_cats  <- standardize_gt(GT_CATS_CSV,  "category")

# Helper: compute metrics per entity & market over last N months
compute_trend_metrics <- function(gt_df, window_months = 12) {
  gt_df %>%
    group_by(entity, entity_type, market) %>%
    mutate(max_date = max(date, na.rm = TRUE),
           cutoff   = max_date %m-% months(window_months - 1)) %>%
    filter(date >= cutoff) %>%
    summarise(
      baseline = mean(interest, na.rm = TRUE),
      pct_change = pct_change_safely(interest),
      slope = slope_raw(date, interest),
      slope_nz = slope_nz(date, interest),
      .groups = "drop"
    ) %>%
    pivot_wider(
      names_from = market,
      values_from = c(baseline, pct_change, slope, slope_nz),
      names_sep = "_"
    ) %>%
    mutate(
      baseline_both = rowMeans(cbind(baseline_US, baseline_UK), na.rm = TRUE),
      pct_change_delta = pct_change_US - pct_change_UK
    )
}

notes_trend <- compute_trend_metrics(gt_notes, trend_window_months)
cats_trend  <- compute_trend_metrics(gt_cats,  trend_window_months)

# ---------------------- 3) Social Buzz -----------------------

read_buzz <- function(path, key_col_guess = c("note","entity","category","key")) {
  if (!file.exists(path)) return(NULL)
  df <- suppressMessages(read_csv(path, show_col_types = FALSE)) %>% clean_names()
  nm <- names(df)
  key_col <- nm[str_detect(nm, paste(key_col_guess, collapse="|"))]
  buzz_col <- nm[str_detect(nm, "buzz")]
  tiktok_col <- nm[str_detect(nm, "tiktok")]
  ig_col <- nm[str_detect(nm, "instagram|ig")]

  if (length(key_col) == 0) return(NULL)

  out <- df %>%
    rename(entity_raw = !!sym(key_col[1])) %>%
    mutate(entity = clean_entity(entity_raw))

  # Prefer an already-computed buzz column if present
  if (length(buzz_col) >= 1) {
    out <- out %>% select(entity, buzz = !!sym(buzz_col[1]))
  } else if (length(tiktok_col) >= 1 || length(ig_col) >= 1) {
    # Example composite: geometric mean, else sum
    out <- out %>%
      mutate(
        tiktok = if (length(tiktok_col)>=1) as.numeric(!!sym(tiktok_col[1])) else NA_real_,
        ig     = if (length(ig_col)>=1)     as.numeric(!!sym(ig_col[1]))     else NA_real_,
        buzz   = ifelse(!is.na(tiktok) & !is.na(ig), sqrt(pmax(tiktok,0) * pmax(ig,0)),
                        coalesce(tiktok, ig))
      ) %>%
      select(entity, buzz)
  } else {
    return(NULL)
  }
  out
}

notes_buzz <- read_buzz(NOTES_MASTER_CSV, c("note","entity","key"))
cats_buzz  <- read_buzz(CATS_MASTER_CSV,  c("category","entity","key"))

# ---------------------- 4) Build TMS (US/UK/Both) ----------------------
build_tms <- function(trend_df, buzz_df = NULL) {
  df <- trend_df %>%
    mutate(
      # Normalize pieces
      g_us  = safe_rescale(pct_change_US),
      g_uk  = safe_rescale(pct_change_UK),
      g_b   = safe_rescale(pct_change_delta),   # delta as a combined signal proxy
      b_us  = safe_rescale(baseline_US),
      b_uk  = safe_rescale(baseline_UK),
      b_b   = safe_rescale(baseline_both)
    )

  if (!is.null(buzz_df)) {
    df <- df %>%
      left_join(buzz_df, by = "entity") %>%
      mutate(buzz_norm = safe_rescale(buzz))
  } else {
    df <- df %>%
      mutate(buzz_norm = NA_real_)
  }

  wg <- weights$w_growth; wb <- weights$w_baseline; ws <- weights$w_buzz

  df %>%
    mutate(
      TMS_US   = coalesce(wg*g_us + wb*b_us + ws*buzz_norm, wg*g_us + wb*b_us, wg*g_us, NA_real_),
      TMS_UK   = coalesce(wg*g_uk + wb*b_uk + ws*buzz_norm, wg*g_uk + wb*b_uk, wg*g_uk, NA_real_),
      TMS_Both = coalesce(wg*g_b  + wb*b_b  + ws*buzz_norm, wg*g_b  + wb*b_b,  wg*g_b,  NA_real_)
    )
}

notes_tms <- build_tms(notes_trend, notes_buzz)
cats_tms  <- build_tms(cats_trend,  cats_buzz)

# --------------------------- 5) Opportunity ----------------------------
# Opp = TMS * (1 - supply_norm)  (per market; Both uses TMS_Both)
# supply_norm = min-max of supply within the domain (notes or categories)

# Prepare supply for notes
notes_supply_df <- note_supply %>%
  transmute(entity = clean_entity(note), supply = as.numeric(supply_count_note))

# Prepare supply for categories
cats_supply_df <- cat_supply %>%
  transmute(entity = clean_entity(category), supply = as.numeric(supply_count_category))

attach_opp <- function(tms_df, supply_df) {
  tms_df %>%
    left_join(supply_df, by = "entity") %>%
    mutate(
      supply = replace_na(supply, 0),
      supply_norm = safe_rescale(supply),
      Opp_US   = TMS_US   * (1 - supply_norm),
      Opp_UK   = TMS_UK   * (1 - supply_norm),
      Opp_Both = TMS_Both * (1 - supply_norm)
    )
}

notes_final <- notes_tms %>% attach_opp(notes_supply_df)
cats_final  <- cats_tms  %>% attach_opp(cats_supply_df)

# ----------------------------- Save ------------------------------------
write_csv(notes_final, file.path(OUT, "notes_trend_tms_opportunity.csv"))
write_csv(cats_final,  file.path(OUT, "categories_trend_tms_opportunity.csv"))

# Also save slim "presentation" extracts
notes_present <- notes_final %>%
  select(entity, entity_type, pct_change_US, pct_change_UK, pct_change_delta,
         baseline_US, baseline_UK, baseline_both,
         TMS_US, TMS_UK, TMS_Both, supply, supply_norm, Opp_US, Opp_UK, Opp_Both) %>%
  arrange(desc(Opp_Both))

cats_present <- cats_final %>%
  select(entity, entity_type, pct_change_US, pct_change_UK, pct_change_delta,
         baseline_US, baseline_UK, baseline_both,
         TMS_US, TMS_UK, TMS_Both, supply, supply_norm, Opp_US, Opp_UK, Opp_Both) %>%
  arrange(desc(Opp_Both))

write_csv(notes_present, file.path(OUT, "notes_presentation_extract.csv"))
write_csv(cats_present,  file.path(OUT, "categories_presentation_extract.csv"))

# ---------------------------- Console Peek -----------------------------
cat("\n=== Top 10 NOTE opportunities (Both) ===\n")
notes_present %>% slice_max(Opp_Both, n = 10) %>%
  mutate(across(where(is.numeric), ~round(.x, 3))) %>%
  print(n = 10)

cat("\n=== Top 10 CATEGORY opportunities (Both) ===\n")
cats_present %>% slice_max(Opp_Both, n = 10) %>%
  mutate(across(where(is.numeric), ~round(.x, 3))) %>%
  print(n = 10)

cat("\nDone. Outputs written to ./outputs\n")